# ğŸ¬ Netflix-Style Content Recommendation Pipeline

This is a full-stack **Data Engineering project** that simulates a streaming service that generates and processes user watch behavior in **real-time** and **batch** using Kafka, Spark, Redis, and FastAPI. Designed to showcase **streaming data pipelines** and **real-time analytics**.

---

## ğŸš€ Architecture Overview

![alt text for screen readers](/kafka_arch.jpg "Streaming architecture")

```

On future modifications, we want to achieve this: (FastAPI to show a dashboard, parquet located in AWS or Azure).

[ User Events ] â†’ Kafka â†’ Spark (Structured Streaming) â†’ Redis â†’ FastAPI (Real-time API)
                          â†“
                       Parquet (HDFS/S3) â†’ Spark Batch â†’ Aggregates
```

---

## ğŸ”§ Tech Stack

- **Kafka**: Real-time event ingestion
- **Zookeeper**: Kafka coordination
- **Spark**: Streaming and batch processing
- **Redis**: Real-time data store for low-latency queries
- **FastAPI**: Lightweight backend for accessing recommendation data
- **Docker Compose**: Service orchestration

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                 # Dockerfile for spark-submit job
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ app/
â”‚           â”œâ”€â”€ stream_watch_events.py  # Spark Structured Streaming job
â”‚           â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚           â”œâ”€â”€ .env                    # Environment variables (Kafka, Redis)
â”‚           â””â”€â”€ simulate_watch_events.py # Event simulator (Kafka producer)
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                     # FastAPI app to query Redis
â””â”€â”€ SPARK-CHECKLIST.md              # Debug checklist for Spark streaming job
```


---

## ğŸ§ª Simulate Watch Events

To simulate watch behavior from users (sends data to Kafka topic):

```bash
python simulate_watch_events.py
```

---

## âš¡ Real-Time Pipeline (Spark Submit)

Launches the Spark streaming job to consume Kafka messages and update Redis:

```bash
docker-compose build spark-submit
docker-compose up spark-submit
```

Make sure topic `watch_events` is created and Redis is reachable.

---

## ğŸŒ FastAPI Real-time API

Runs a simple REST API to query Redis-stored insights:

```bash
uvicorn main:app --reload --port 8000
```

- `GET /recommendations/{user_id}` â†’ Returns list of recommendations

---

## ğŸ“ .env Configuration

Inside `spark/app/.env`:

```env
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC=watch_events
REDIS_HOST=redis
REDIS_PORT=6379
```

---

## ğŸ“„ Sample Kafka Topic Creation (Optional)

Create topic manually:

```bash
docker exec -it kafka kafka-topics --create --topic watch_events   --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
```

---

## âœ… Health Checklist

See [SPARK-CHECKLIST.md](SPARK-CHECKLIST.md) for end-to-end testing, log tracing, and common errors.

---


## ğŸ“¬ Future Improvements

- Add PostgreSQL or BigQuery as offline warehouse
- Real-time user profiling with Redis TTLs
- Stream processing metrics with Prometheus/Grafana

---

## ğŸ“˜ License

MIT License.