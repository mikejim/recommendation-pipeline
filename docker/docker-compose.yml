version: '3.7'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  redis:
    image: redis:7
    ports:
      - "6379:6379"

  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
    volumes:
      - ./shared_volume:/app/shared_volume

  spark-worker:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./shared_volume:/app/shared_volume

  spark-submit:
    user: "0:0"
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - kafka
      - redis
      - spark-master
    volumes:
      - ./shared_volume:/app/shared_volume
      - ./log4j.properties:/opt/bitnami/spark/conf/log4j.properties
      #- ./spark/app:/opt/spark-app
      # Windows-compatible volume mount - use absolute path from project root
      #- ./data:/opt/spark-app/data
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - PYTHONPATH=/opt/spark-app
    entrypoint: ["/bin/bash", "/app/entrypoint.sh"]
    command: >
      bash -c "sleep 30 && /opt/bitnami/spark/bin/spark-submit
      --master spark://spark-master:7077
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1
      /opt/spark-app/stream_watch_events.py"
  dash-app:
    build: .
    container_name: dash-app
    ports:
      - "8050:8050"
    volumes:
      - ./shared_volume:/app/shared_volume
      - ./dash-app:/app/dash-app   
      - ./entrypoint_dash.sh:/app/entrypoint_dash.sh
    entrypoint: ["/bin/bash", "/app/entrypoint_dash.sh"]
    depends_on:
      - spark-submit